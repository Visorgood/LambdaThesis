\section{Design of the System [VI]}

Architecture of our system essentially repeats common design of the Speed layer, described in the Section~\ref{sec:speed_layer}.
The system consists of three main components: data source, data processing, and data storage.
Figure~\ref{fig:SpeedLayerArchitecture} depicts the general structure of the system.
Data source component is essentially beyond the inner architecture of the system.
Nevertheless, we consider it as a part, because it provides data, and we use specific libraries and classes to work with it.
Data processing component, implemented in two ways, using Storm or Spark Streaming, is the core of the system.
It executes computations, and stores results to the data storage for further use.
Data storage component, that uses Redis key-value store, is a temporal database for maintaining real-time views.
Those views are then accessed by the Serving layer for query answering.

\begin{figure}[h]
  \centering
  \includegraphics [width=1.0\textwidth]{images/SpeedLayerArchitecture}
  \caption{General structure of our system.}
  \label{fig:SpeedLayerArchitecture}
\end{figure}

\subsection{Data Source}

The source of data, that is being processed, is the Kafka queue server \ref{subs:kafka}.
In its turn, it receives data from the outer sources, specifically from smartphones.
There are so far tens of thousands of smartphones, that use Menthal application and send data to the server.
Their number can grow arbitrarily, what requires in essence to have such a complex distributed server.
Smartphones send data to the server in small batches, that contain about fifty events, that happened since the last sending.
Kafka queue server stores this data temporarily, and tries to send it to all consumers, until they acknowledge delivery.
Our system is one of the consumers, and when it receives data, it starts processing. 

For each event type Kafka queue server maintains a dedicated topic.
It is possible then to subscribe for receiving of those events, that are needed for processing.
Events are sent as Avro objects \ref{subs:avro}, and contain meaningful information about what user does on the phone.
In our case we consider event types presented in the Table~\ref{table:event_types}.
In the future this list can be changed or extended.
Moreover, the event type detection can be done in different way.
For instance, the information about the event type can be carried in a message header.
However, currently the infrastructure of our project is constructed in a way that the message type is determined by the Kafka topic it goes from.

\begin{table}[h]
\begin{tabular}{ | l | p{10cm} |}
    \hline
    AppInstall & Contains user id, app name and timestamp. Generated when the user installed a new application. \\ \hline
    AppSession & Contains user id, app name, timestamp and duration. Generated when the user has finished a session of using application. \\ \hline
    CallMissed & Contains user id, timestamp and contact hash. Generated when the user missed a call. Contact hash specifies a contact, that called. \\ \hline
    CallOutgoing & Contains user id, timestamp, contact hash and duration. Generated when the user finished an outgoing call. Contact hash specifies a contact, to that the call was addressed. \\ \hline
    CallReceived & Contains user id, timestamp, contact hash and duration. Generated when the user finished an incoming call. Contact hash specifies a contact, that called. \\ \hline
    DreamingStarted & Contains user id and timestamp. Generated when the phone gone to the dreaming mode. \\ \hline
    DreamingStopped & Contains user id and timestamp. Generated when the phone woke up from the dreaming mode. \\ \hline
    PhoneShutdown & Contains user id and timestamp. Generated when the phone goes off. \\ \hline
    ScreenOff & Contains user id and timestamp. Generated when the phone's screen goes off. \\ \hline
    ScreenOn & Contains user id and timestamp. Generated when the phone's screen goes on. \\ \hline
    ScreenUnlock & Contains user id and timestamp. Generated when the phone's screen is unlocked. \\ \hline
    SmsReceived & Contains user id,  timestamp, contact hash and message length. Generated when an sms is received. Contact hash specifies the contact, from that sms has come. \\ \hline
    SmsSent & Contains user id,  timestamp, contact hash and message length. Generated when an sms is sent. Contact hash specifies the contact, to that sms was sent. \\ \hline
    WindowStateChanged & Contains user id, timestamp, app name and window title. Generated when the new window becomes active. \\
    \hline
\end{tabular}
\caption{Event types and their descriptions}
\label{table:event_types}
\end{table}

\mnote{Kafka spout}
In Storm, to obtain events from the Kafka queue server, we use \textit{Kafka spouts}.
Kafka spout is the class in the Storm's external class library, that allows to use Kafka queue server as a source of data for processing in the topology.
It listens for messages coming from Kafka in a particular topic.
Each such spout works in a distributed fashion, so that no matter how many events of a particular type will come, we can overcome this simply adding more machines to the cluster.

Spark does not have such a concept like a spout, but it has its own mechanism of integration with Kafka.
Spark uses a \textit{DStream} that is created via \textit{KafkaUtils.createStream}, that receives data from Kafka topics.
Similar to the Storm case, we also create one DStream for each Kafka topic.
Because of this we have a possibility to handle each type of events separately.

\subsection{Data Processing}

The data processing part is the main component of the system.
We have two different implementations of this component, one is written using Storm framework, while another is based on Spark Streaming.
We tried both of the technologies to compare them and to choose one that best fits our needs.
Data processing component receives messages from Kafka queue server, and performs different computations on that data.
Its purpose is to provide different aggregations on data coming from smartphones.
These aggregations are then stored in the storage system, and can be accessed by the Speed layer for answering queries.

\mnote{Storm implementation}
The core of Storm \ref{subs:storm} framework is topology.
Topology describes the flow of computational logic.
It contains spouts and bolts, that represent data sources and data processing nodes, respectively.
We have already discussed Kafka spout, that we use for receiving data from the Kafka queue server.

Our topology has simple structure, but nevertheless has many nodes inside.
First of all, for each event type there is dedicated Kafka spout, that receives data from the Kafka queue server.
For that sake each Kafka spout is subscribed for a specific Kafka topic, corresponding to that event type.
When new message comes from the Kafka queue server, Storm runs particular spout on any available cluster node.

\mnote{Spark implementation}
Spark has another conception, different from Storm.
In our project we use Spark Streaming - technology that allows to handle data in real time.
Thus the main notion in Spark implementation is a \textit{discretized stream} (DStream).
Later on this stream is divided into batches and they are processed by a Spark engine.
The Spark DStream can be considered as an analogue of a spout in Storm in a context of our work.
Therefore, similarly to Storm, every Kafka topic has a corresponding DStream in Spark.
In Storm we run a Topology, here we run a StreamingContext.

In the case of Spark Streaming, we have to deal with its Resilient Distributed Datasets (RDD).
RDD represents the Spark (not Spark Streaming) main concept, that allows to perform batch processing.
To provide near real-time processing, Spark Streaming uses DStreams as sources of data.
Each DStream than divided by Spark into a sequence of RDDS.
From this moment Spark can perform the same operations on RDDs as it uses in batch processing.
In our case we use \textit{foreachRDD} as an output operation on RDDs.
This function triggers the computation of the corresponding stream.
RDD is a distributed collection, and to execute a peace of code, the Spark driver node serializes this code and sends to the workers, where it can be deserialized and executed.
Our problem is that we use several classes to process the input data and store the results in Redis, that are not serializable.
Therefore we have to be careful with the place where the code, that calls the functions from nonserializable classes, is placed.
The best solution in our case is to use \textit{foreachPartition} method, that is called for each RDD.
As a result, we create an instance of this nonserializable class for the whole partition and then apply the needed operations on each record of this partition. 

The rest parts of our system are very similar for Storm and Spark implementations, apart from the terminology that we have to use.
To avoid the redundancy, we describe below only the Storm case.
The same description can be applied to the Spark case, by replacing a \textit{bolt} notion to a \textit{stream} notion. 

There is a dedicated bolt for each event type in our topology.
For each bolt we create a dedicated class, that performs particular processing, depending on what event type is it.
We use Storm's class \lstinline{BaseRichBolt} as a base class for all our bolts.
Our abstract class \lstinline{EventProcessingBolt}, that extends \lstinline{BaseRichBolt}, provides base functionality for all other bolts.
Every particular bolt class implements abstract method \lstinline{processEvent}, given in the class \lstinline{EventProcessingBolt}.
Such class hierarchy allows to add new event types for processing easily.
We just need to create new class, that inherits from \lstinline{EventProcessingBolt} and implements method \lstinline{processEvent}.

Class \lstinline{EventProcessingBolt} has a method \lstinline{getEventProcessingBoltByEventName}, that creates new bolt class by its name.
To achieve such functionality, class \lstinline{EventProcessingBolt} has protected field \lstinline{schemaName}.
It must be initialized in each derived class with the name of this particular event type.
This is useful again for the sake of extensibility, because it let to leave code of topology initialization the same, no matter how many new event types we add for processing.
The main method of the \lstinline{EventProcessingBolt}, namely \lstinline{execute}, is presented on the Listing~\ref{listing:EventProcessingBolt_execute}.

\begin{lstlisting}[float=h, caption=The main method of the EventProcessingBolt., label=listing:EventProcessingBolt_execute, language=Java]
public void execute(Tuple tuple) {
  try {
    Schema schema = new Schema.Parser().parse(new File(schemaName + ".avsc"));
    DatumReader<GenericRecord> datumReader = new GenericDatumReader<GenericRecord>(schema);
    InputStream in = new ByteArrayInputStream((byte[])tuple.getValue(0));
    GenericRecord record = datumReader.read(null, DecoderFactory.get().jsonDecoder(schema, in));
    processEvent(record);
    _collector.emit(new Values(record));
  } catch (Exception e) {
    System.out.println("Exception raised!");
    System.out.println(e.getMessage());
  }
}
\end{lstlisting}

In the Line 3 we parse the schema of that particular event type.
Then in Lines 4-6 we parse input event to a \lstinline{GenericRecord} object using that schema.
\lstinline{GenericRecord} is an Avro class for representation of Avro objects in memory.
In the Line 7 we call abstract method \lstinline{processEvent}, that executes particular processing depending on what exact bolt class is it.

\lstinline{EventProcessingBolt} holds a reference to an interface \lstinline{EventAggregator}.
This interface provides method for processing all of event types that we consider.
Listing~\ref{listing:EventAggregator} shows the snippet from the definition of that interface.
We implemented exact class \lstinline{RedisEventAggregator} that realizes this interface.
As it states in its name, it works with Redis data storage.

\begin{lstlisting}[float=h, caption=The partial listing of the interface EventAggregator., label=listing:EventAggregator, language=Java]
public interface EventAggregator {
  void processAppSession(long userId, long time, long duration, String appName);
  void processCallMissed(long userId, long time, String contactHash, long timestamp);
  void processScreenOn(long userId, long time);
  void processSmsSent(long userId, long time, String contactHash, int msgLength);
  void processWindowStateChanged(long userId, long time, String appName, String windowTitle);
  ...
}
\end{lstlisting}

To make an example, let us consider one particular bolt class - \lstinline{AppSessionBolt}.
Listing~\ref{listing:AppSessionBolt} presents it.
In the constructor we initialize \lstinline{schemaName} with the name of this event type.
It is useful for creation of bolts by name.
In the method \lstinline{processEvent} we print out given record, then retrieve data fields from it, and pass them to a specific method of the \lstinline{eventAggregator} object, called \lstinline{processAppSession}.

\begin{lstlisting}[float=h, caption=Implementation of AppSessionBolt class., label=listing:AppSessionBolt, language=Java]
public class AppSessionBolt extends EventProcessingBolt {
  public AppSessionBolt() {
    schemaName = "app_session";
  }

  @Override
  protected void processEvent(GenericRecord record) {
    System.out.println(schemaName + "-Bolt: " + record.toString());
    long userId = (long)record.get("userId");
    long time = (long)record.get("time");
    long duration = (long)record.get("duration");
    String appName = record.get("appName").toString();
    eventAggregator.processAppSession(userId,time,duration,appName);
  }
}
\end{lstlisting}

On the events coming from smartphones we do different aggregations.
They will be then taken by the Serving layer for answering queries.
There are three types of aggregation, that we maintain in the data storage: counter, duration, and length.
Counter collects the number of events of any particular type.
Duration accumulates the total duration of the continuing action on the smartphone.
Length contains the total length of sms or whatsapp messages, etc.
There is a different logic of how to compute them for these three types of aggregations. 
For each aggregation we store four cases for different time intervals: last hour, last day, last week, last month.
Specifically, each aggregations consists of two values: timestamp of the start of the current hour/day/week/month, and aggregation value itself.
Each time we update aggregation, that requires to recompute its starting timestamp.

\subsection{Data Storage}

Data storage Redis \ref{subs:redis} keeps all results of computations.
These results are different aggregations, specifically counters, that are stored in the complex data model.
Redis provides simple access to save and then get that data.
Counters, saved in the Redis data storage can be then used by the Speed layer to merge them with the results of batch computations.

Redis is a key-value store, what defines the approach of storing data.
Each aggregations that we save to Redis is a list of two values: timestamp and value.

To work with aggregations in Redis we have developed a concrete class \lstinline{RedisEventAggregator}, that implements an interface \lstinline{EventAggregator}.
It works with the object of the type \lstinline{RedisProxy}, that we discribe later on in this section.
Each method of the class \lstinline{RedisEventAggregator} receives arguments as for example: user id, timestamp, app name, etc.
We then combine these values to a proper key name, that is present in Redis (or must be created on the first update).
Listing~\ref{listing:processAppSession} shows an example of key creation and passing it to \lstinline{RedisProxy}.

\begin{lstlisting}[float=h, caption=Example of key creation for aggregation update., label=listing:processAppSession, language=Java]
public void processAppSession(long userId, long time, long duration, String appName) {
  String key = String.format("app:%s:%s", appName, "sessions");
  redisProxy.incrementCounters(key, time);
  key = String.format("app:%s:%s", appName, "total_time");
  redisProxy.incrementDurations(key, time, duration);
  ...
\end{lstlisting}

We keep keys in Redis in the following way.
Each key is associated with the list, that has two elements.
The first one is the timestamp of the beginning of counting.
The second one is the value itself.
There are several examples of the keys in the Listing~\ref{listing:keysInRedis}.

\begin{lstlisting}[float=h, caption=Examples of the keys in Redis., label=listing:keysInRedis]
user:$user_id:$phone_hash:incoming_msg_count:count:hourly
user:$user_id:$phone_hash:incoming_msg_count:count:daily
user:$user_id:$phone_hash:incoming_msg_count:count:weekly
user:$user_id:$phone_hash:incoming_msg_count:count:monthly
user:$user_id:$phone_hash:outgoing_call_duration:duration:hourly
user:$user_id:$phone_hash:outgoing_call_duration:duration:daily
user:$user_id:$phone_hash:outgoing_call_duration:duration:weekly
user:$user_id:$phone_hash:outgoing_call_duration:duration:monthly
user:$user_id:$phone_hash:outgoing_msg_length:length:hourly
user:$user_id:$phone_hash:outgoing_msg_length:length:daily
user:$user_id:$phone_hash:outgoing_msg_length:length:weekly
user:$user_id:$phone_hash:outgoing_msg_length:length:monthly
\end{lstlisting}

To access Redis we use Jedis java library \cite{Jedis}.
It provides the whole functionality, that Redis offers itself.
The class \lstinline{RedisProxy} contains all the communication with Redis.
It contains two fields, presentd on the Listing~\ref{listing:RedisProxyFields}.
They are the part of the Jedis library, and provide exact access to the Redis database.

\begin{lstlisting}[float=h, caption=Two main fields of the class RedisProxy., label=listing:RedisProxyFields, language=Java]
private final Jedis jedis;
private Pipeline pipeline;
\end{lstlisting}

\mnote{Pipeline}
We use \textit{pipelines} to communicate with Redis database.
Pipeline allows to combine several requests to Redis into one batch, and send it altogether to the server through the network.
It reduces network congestion, and makes probability of race conditions on the server less.
\mnote{Transaction}
Inside of pipeline we use \textit{transaction} to tell Redis server, that we this set of commands must be executed as a one atomic command.
This is important to avoid race conditions, because many clients in the same time can try to access the same keys in the Redis database.
We demonstrate changing of the counter on the Listing~\ref{listing:incrementCounter}.

\begin{lstlisting}[float=h, caption=Example of updating counter aggregation in the Redis database., label=listing:incrementCounter, language=Java]
private void incrementCounter(DurationType durationType, CounterType counterType, String key, long time, long valueToIncrement) {
  key = String.format("%s:%s:%s", key, getCounterName(counterType), getDurationName(durationType));
  Boolean success = false;
  while (!success) {
    jedis.watch(key);
    List<String> value = jedis.lrange(key, 0, 1);
    pipeline = jedis.pipelined();
    pipeline.multi();
    Counter counter = new Counter(durationType, Long.parseLong(value.get(0)), Long.parseLong(value.get(1)));
    if (updateStartingTime(counter, time, 0))
      pipeline.lset(key, 0, Long.toString(counter.startTime));
    counter.value += valueToIncrement;
    pipeline.lset(key, 1, Long.toString(counter.value));
    success = (pipeline.exec() != null);
  }
  pipeline.sync();
}
\end{lstlisting}

We first generate in the Line 2 a proper key, that exactly exists in Redis, from a base key part.
Then we do watching of values, that we want to retrieve and then change.
This is necessary to do, because this is the only way to protect the whole operation from race condition on the server side.
We get value of the counter in the Line 6.
Then in the Lines 7-8 we initialize pipeline and transaction.
In the Lines 9-12 we update retrieved counter in memory.
And in the Line 13 we set the new value of the counter into Redis server.
If in the time between watch and update of key's value it was accessed by other client, operation will fail, and we try to execute it again.