Part III Big Data Arcitecture

In the modern world the p  henomena of Big Data becomes more and more wide-spread. Every second terabytes of data are created and need to be stored and processed. Flow of information originates from all the spheres of human activities, starting from information transmitted from satellites, financial transactions among organizations, banks and broker-dealers, and ending with published data in social networks, blogs, message exchange between Internet users. Moreover, this flow increases significantly every year. That is the reason why the concept of Big Data appeared. There is no consensus about the origins of the term "Big Data", but most of the sources claim that it is first mentioned in the press in 2008. People start actively using it since 2009 and it spreads quickly owing to its precise and capacious meaning. Basically, characteristic properties of Bid Data are its huge volume, the high speed of its transmission and the variety of data sources. In 2010 the biggest IT corporations start their own research in this area, generating new technological solutions to the problems of Big Data.  [reference?] Google is one of the well-known examples of such corporations. Its activity directly relates to storing and processing of Big Data, therefore Google works on a variety of technologies, such as Google File System, Big Table, etc. to handle these problems. Let us explain in more detail the primary features and internal structure of these technologies.

Google File System (GFS) is a scalable distributed file system, which supports Big Data operations. The underlying idea is the following: Google Search Engine and some other Google systems process vast amount of data, which is spread all over the world. Hence the file system should be highly extensible, fault tolerate and give an opportunity to use cheap hardware components. Furthermore, it has some specific usage features. For example, most of the time the stored data stays unchanged and new data is only appended. GFS architecture design helps to meet all these requirements. Each GFS cluster contains one master server and several chunkservers. Each chunk, in its turn, combines multiple files. The master node manages the mapping from files to chunks, location of chunks, access control, garbage collection and some other tasks. To prevent the master being a bottleneck, clients pass through it only file system control data, e.g. a client can ask which chunkservers it should contact. After receiving a reply, the client directly transfers data to the given chunkserver, dispensing master node from overload. 

It is important to mention two fundamentally different ways of processing the Big Data, namely Batch and Real Time data processing. In the first case, data is handled in batches, i.e. process collects the data until the batch size is obtained, and only after this the process can perform the necessary actions on a batch as a whole. Hadoop is one of the technologies, which are focused on batch data processing. On the other hand, the main idea of real time data processing, as it follows from the name, that the data is handled at the moment of arrival. The advantage of the latter is that the results are available almost immediately, which can be an essential requirement in such areas as medicine or self-driving car technologies. 

• Big Data Definition

• other architectures