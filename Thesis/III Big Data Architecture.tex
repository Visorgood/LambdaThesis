Part III Big Data Arcitecture

In the modern world the p  henomena of Big Data becomes more and more wide-spread. Every second terabytes of data are created and need to be stored and processed. Flow of information originates from all the spheres of human activities, starting from information transmitted from satellites, financial transactions among organizations, banks and broker-dealers, and ending with published data in social networks, blogs, message exchange between Internet users. Moreover, this flow increases significantly every year. That is the reason why the concept of Big Data appeared. There is no consensus about the origins of the term "Big Data", but most of the sources claim that it is first mentioned in the press in 2008. People start actively using it since 2009 and it spreads quickly owing to its precise and capacious meaning. Basically, characteristic properties of Bid Data are its huge volume, the high speed of its transmission and the variety of data sources. In 2010 the biggest IT corporations start their own research in this area, generating new technological solutions to the problems of Big Data.  [reference?] Google is one of the well-known examples of such corporations. Its activity directly relates to storing and processing of Big Data, therefore Google works on a variety of technologies, such as Google File System, Big Table, etc. to handle these problems. Let us explain in more detail the primary features and internal structure of these technologies.

Google File System (GFS) is a scalable distributed file system, which supports Big Data operations. The underlying idea is the following: Google Search Engine and some other Google systems process vast amount of data, which is spread all over the world. Hence the file system should be highly extensible, fault tolerate and give an opportunity to use cheap hardware components. Furthermore, it has some specific usage features. For example, most of the time the stored data stays unchanged and new data is only appended. GFS architecture design helps to meet all these requirements. Each GFS cluster contains one master server and several chunkservers. Each chunk, in its turn, combines multiple files. The master node manages the mapping from files to chunks, location of chunks, access control, garbage collection and some other tasks. To prevent the master being a bottleneck, clients pass through it only file system control data, e.g. a client can ask which chunkservers it should contact. After receiving a reply, the client directly transfers data to the given chunkserver, dispensing master node from overload. 

It is important to mention two fundamentally different ways of processing the Big Data, namely Batch and Real Time data processing. In the first case, data is handled in batches, i.e. process collects the data until the batch size is obtained, and only after this the process can perform the necessary actions on a batch as a whole. Hadoop is one of the technologies, which are focused on batch data processing. On the other hand, the main idea of real time data processing, as it follows from the name, that the data is handled at the moment of arrival. The advantage of the latter is that the results are available almost immediately, which can be an essential requirement in such areas as medicine or self-driving car technologies. 

• Big Data Definition

• other architectures

The process of making decisions is a liable, time consuming and complicated procedure. To clear up the situation it is better to use an illustrative example. Let us take a marketing specialist. His main professional goal is to increase sales growth rate and extend a client base. To achieve this goal it is necessary to choose the best advertising strategy. To do that a marketing specialist needs to make a decision about the interests of a potential customer. It is important to choose not only the product a customer is interested in, but also the proper way of presenting information to maximally attract a customer's attention and finally persuade him to make a purchase. A long history of market economy creates a variety of solutions to this problem. The straightforward solution is to try to promote one product and then, basing on the analysis of obtained results, improve the original strategy. However, this can lead to high expenses, never reaching the optimal result. The better way is to make a prior research. For example, it is possible to carry out a survey to find out the customers' interests, or to choose a control group of potential customers and run experiments on it. The disadvantage of this approach is that both surveys and experiments on a control group not necessary give the valid estimate about the potential customer's preferences. A lot of various reasons can cause such problem, such as a survey composed in a wrong way, or a not representative control group. Owing to the recent progress in the information technologies area, we obtain an alternative solution to this problem, which avoids the drawbacks of the strategies presented above. We can start with gathering of all accessible information about each potential customer: age, marriage status, geographical position, visited websites, made purchases, clicked links, the time of performed actions, etc. As a result we have a huge amount of data, which, with the right approach, can give lots of useful information about the person's habits and preferences. To avoid making unsubstantiated statements, let us refer to a recent incident covered in the news. Analytical experts from retailing company Target, using the approach mentioned above, managed to detect the fact of pregnancy of a schoolgirl even before her parents found it out. Just relying on the analysis of the gathered data, they made a correct conclusion and started to show her the appropriate advertisement. [http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html?pagewanted=1&_r=3&hp&]
The constant reduction of data storage costs and rapid progress in analytics, machine learning algorithms and data mining allow to collect, store and process vast amounts of data. And the example mentioned in the previous paragraph is not the only source of such data. Flow of information originates from all the spheres of human activities, starting from information transmitted from satellites, financial transactions among organizations, banks and broker-dealers, and ending with published data in social networks, blogs, message exchange between Internet users. Moreover, this flow increases significantly every year. This data can be collected not only on purpose, but also it can appear in many cases to be just a by-product of other processes. Nowadays people refer to such data using a special term - Big Data. There is no consensus about the origins of the term "Big Data", but most of the sources claim that it is first mentioned in the press in 2008. People start actively using it since 2009 and it spreads quickly owing to its precise and capacious meaning. 
Basically, characteristic properties of Bid Data are its huge volume, the high speed of its transmission and the variety of data sources. [Laney, Douglas. "3D Data Management: Controlling Data Volume, Velocity and Variety". Gartner. Retrieved 6 February 2001.] Due to the origin of Big Data it always has enormous size. For instance, by the end of 2013, the number of Facebook users reaches 1.23 billion. Each of them not only has some profile information, but also communicates with other users, shares data, updates the timeline and so forth. As a result, Facebook faces a problem of storing and processing petabytes of data. Another example is the information received from all types of sensors. Sensor is a converter that measures and transforms physical quantity into a digital signal. Sensors find an application in various fields: manufacturing industry, transportation systems, meteorology, medicine, even a modern smartphones have lots of sensors. The key feature of a sensor is that often it does its job constantly, continuously producing the flow of information, what leads to large volumes of data. Menthal - number of users, amount of data?
On the other hand, the velocity of Big Data transmission and processing should be high. 



